{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your task is as follows:\n",
    "- read the provided Excel file\n",
    "- find and return the min, max and average values for the COAST region\n",
    "- find and return the time value for the min and max entries\n",
    "- the time values should be returned as Python tuples\n",
    "\n",
    "Please see the test function for the expected return format\n",
    "\"\"\"\n",
    "import xlrd\n",
    "from zipfile import ZipFile\n",
    "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "\n",
    "\n",
    "def open_zip(datafile):\n",
    "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "\n",
    "    ### example on how you can get the data\n",
    "    sheet_data = [[sheet.cell_value(r, col) for col in range(sheet.ncols)] for r in range(sheet.nrows)]\n",
    "    \n",
    "    ### other useful methods:\n",
    "    # print \"\\nROWS, COLUMNS, and CELLS:\"\n",
    "    # print \"Number of rows in the sheet:\", \n",
    "    # print sheet.nrows\n",
    "    # print \"Type of data in cell (row 3, col 2):\", \n",
    "    # print sheet.cell_type(3, 2)\n",
    "    # print \"Value in cell (row 3, col 2):\", \n",
    "    # print sheet.cell_value(3, 2)\n",
    "    # print \"Get a slice of values in column 3, from rows 1-3:\"\n",
    "    # print sheet.col_values(3, start_rowx=1, end_rowx=4)\n",
    "\n",
    "    # print \"\\nDATES:\"\n",
    "    # print \"Type of data in cell (row 1, col 0):\", \n",
    "    # print sheet.cell_type(1, 0)\n",
    "    # exceltime = sheet.cell_value(1, 0)\n",
    "    # print \"Time in Excel format:\",\n",
    "    # print exceltime\n",
    "    # print \"Convert time to a Python datetime tuple, from the Excel float:\",\n",
    "    # print xlrd.xldate_as_tuple(exceltime, 0)\n",
    "    \n",
    "    data = {\n",
    "            'maxtime': (0, 0, 0, 0, 0, 0),\n",
    "            'maxvalue': 0,\n",
    "            'mintime': (0, 0, 0, 0, 0, 0),\n",
    "            'minvalue': 0,\n",
    "            'avgcoast': 0\n",
    "    }\n",
    "    \n",
    "    coast_data = sheet.col_values(1, start_rowx=1, end_rowx=None)\n",
    "    data['maxvalue'] = max(coast_data)\n",
    "    idx_maxvalue = coast_data.index(max(coast_data))\n",
    "    data['maxtime'] = xlrd.xldate_as_tuple(sheet.cell_value(idx_maxvalue+1, 0), 0)\n",
    "    data['minvalue'] = min(coast_data)\n",
    "    idx_minvalue = coast_data.index(min(coast_data))\n",
    "    data['mintime'] = xlrd.xldate_as_tuple(sheet.cell_value(idx_minvalue+1, 0), 0)\n",
    "    data['avgcoast'] = sum(coast_data)/float(len(coast_data))\n",
    "    return data\n",
    "\n",
    "\n",
    "def test():\n",
    "    #open_zip(datafile)\n",
    "    data = parse_file(datafile)\n",
    "\n",
    "    assert data['maxtime'] == (2013, 8, 13, 17, 0, 0)\n",
    "    assert round(data['maxvalue'], 10) == round(18779.02551, 10)\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requesting http://musicbrainz.org/ws/2/artist/?query=artist%3AOne+Direction&fmt=json\n",
      "{\n",
      "    \"artists\": [\n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"8a754a16-0027-3a29-b6d7-2b40ea0481ed\", \n",
      "                \"name\": \"United Kingdom\", \n",
      "                \"sort-name\": \"United Kingdom\"\n",
      "            }, \n",
      "            \"begin-area\": {\n",
      "                \"id\": \"f03d09b3-39dc-4083-afd6-159e3f0d462f\", \n",
      "                \"name\": \"London\", \n",
      "                \"sort-name\": \"London\"\n",
      "            }, \n",
      "            \"country\": \"GB\", \n",
      "            \"disambiguation\": \"English-Irish boy band formed in 2010\", \n",
      "            \"id\": \"1a425bbd-cca4-4b2c-aeb7-71cb176c828a\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"2010-07\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"One Direction\", \n",
      "            \"score\": \"100\", \n",
      "            \"sort-name\": \"One Direction\", \n",
      "            \"tags\": [\n",
      "                {\n",
      "                    \"count\": 2, \n",
      "                    \"name\": \"pop\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"power pop\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"dance-pop\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"pop rock\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"folk pop\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 2, \n",
      "                    \"name\": \"boy band\"\n",
      "                }\n",
      "            ], \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"489ce91b-6658-3307-9877-795b68554c98\", \n",
      "                \"name\": \"United States\", \n",
      "                \"sort-name\": \"United States\"\n",
      "            }, \n",
      "            \"country\": \"US\", \n",
      "            \"disambiguation\": \"San Francisco band\", \n",
      "            \"id\": \"d3479e62-76ac-4aec-9f95-5b222d1e26b1\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Direction\", \n",
      "            \"score\": \"69\", \n",
      "            \"sort-name\": \"Direction\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"b2bc1294-77be-4c7b-af93-9868b83b1f34\", \n",
      "                \"name\": \"Auckland\", \n",
      "                \"sort-name\": \"Auckland\"\n",
      "            }, \n",
      "            \"disambiguation\": \"New Zealand punk?\", \n",
      "            \"id\": \"a234ebb0-0399-4253-baf6-2ae0abc9d8f2\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Direction\", \n",
      "            \"score\": \"47\", \n",
      "            \"sort-name\": \"Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"ef1b7cc0-cd26-36f4-8ea0-04d9623786c7\", \n",
      "                \"name\": \"Netherlands\", \n",
      "                \"sort-name\": \"Netherlands\"\n",
      "            }, \n",
      "            \"begin-area\": {\n",
      "                \"id\": \"0b565bc6-05e7-4c3a-bc28-075fabd9a8d0\", \n",
      "                \"name\": \"Maastricht\", \n",
      "                \"sort-name\": \"Maastricht\"\n",
      "            }, \n",
      "            \"country\": \"NL\", \n",
      "            \"id\": \"574236dc-aa46-4194-8200-6cb039e9ddbd\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"1989\", \n",
      "                \"end\": \"2008\", \n",
      "                \"ended\": true\n",
      "            }, \n",
      "            \"name\": \"Right Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Right Direction\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"cfd2b787-3656-44ce-9212-9e555b400af5\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"New Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"New Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"a32cc520-f2e6-4db2-8b61-c7baca599ef5\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Second Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Second Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"2524b4ea-965f-4628-b940-acd0a9210047\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Lost Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Lost Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"75fb2c35-4f3a-4145-b948-da871176f039\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Disco Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Disco Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"9e9d35c3-ce54-4a48-b3ba-4eebdadc1ed8\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Audio Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Audio Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"355fa6a9-dba2-43b0-bd0f-a9e6dc14c79f\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Direction X\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Direction X\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"7c4e4dc0-6a0e-4887-b374-0cc57749f563\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Direction Indicator\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Direction Indicator\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"74c2fbfc-0ced-4f8a-9da6-7df628755757\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"No Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"No Direction\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"6130ddae-22dd-4930-ba0f-6d59f81d3637\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"2009-08\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"The Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Direction, The\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"489ce91b-6658-3307-9877-795b68554c98\", \n",
      "                \"name\": \"United States\", \n",
      "                \"sort-name\": \"United States\"\n",
      "            }, \n",
      "            \"country\": \"US\", \n",
      "            \"disambiguation\": \"USA punk band\", \n",
      "            \"id\": \"d554887e-2e17-48f4-8610-624a944a9329\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"No Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"No Direction\", \n",
      "            \"tags\": [\n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"punk\"\n",
      "                }, \n",
      "                {\n",
      "                    \"count\": 1, \n",
      "                    \"name\": \"usa\"\n",
      "                }\n",
      "            ], \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"ce85410c-10e4-4b2e-a878-72483fcf0884\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"7th Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"7th Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"47813cd0-be78-48ab-802f-1306fc180124\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Loose Direction\", \n",
      "            \"score\": \"43\", \n",
      "            \"sort-name\": \"Loose Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"65cd1bef-0de3-439e-a2fc-a71c308c6359\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"\\u9ad8\\u67f3\\u660c\\u884c New Direction\", \n",
      "            \"score\": \"38\", \n",
      "            \"sort-name\": \"Masayuki Takayanagi New Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"aliases\": [\n",
      "                {\n",
      "                    \"begin-date\": null, \n",
      "                    \"end-date\": null, \n",
      "                    \"locale\": null, \n",
      "                    \"name\": \"Masayuki \\\"Jojo\\\" Takayanagi* & New Direction For The Arts\", \n",
      "                    \"primary\": null, \n",
      "                    \"sort-name\": \"Masayuki \\\"Jojo\\\" Takayanagi* & New Direction For The Arts\", \n",
      "                    \"type\": null\n",
      "                }\n",
      "            ], \n",
      "            \"area\": {\n",
      "                \"id\": \"2db42837-c832-3c27-b4a3-08198f75693c\", \n",
      "                \"name\": \"Japan\", \n",
      "                \"sort-name\": \"Japan\"\n",
      "            }, \n",
      "            \"country\": \"JP\", \n",
      "            \"id\": \"8367123e-9a2b-4f1a-ad22-52dee85b144a\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"New Direction for the Arts\", \n",
      "            \"score\": \"36\", \n",
      "            \"sort-name\": \"New Direction for the Arts\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"3ccdd708-a6f2-4117-938d-4b23f045cbbb\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"The New Transit Direction\", \n",
      "            \"score\": \"34\", \n",
      "            \"sort-name\": \"New Transit Direction, The\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"71bbafaa-e825-3e15-8ca9-017dcad1748b\", \n",
      "                \"name\": \"Canada\", \n",
      "                \"sort-name\": \"Canada\"\n",
      "            }, \n",
      "            \"country\": \"CA\", \n",
      "            \"id\": \"e0027978-441c-4877-b36f-0d2833181a42\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"The Beat Direction\", \n",
      "            \"score\": \"34\", \n",
      "            \"sort-name\": \"Beat Direction, The\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"disambiguation\": \"rock\", \n",
      "            \"id\": \"dffb4132-bfd1-4c23-8128-31c23d0b6fe9\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Found My Direction\", \n",
      "            \"score\": \"34\", \n",
      "            \"sort-name\": \"Found My Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"area\": {\n",
      "                \"id\": \"85752fda-13c4-31a3-bee5-0e5cb1f51dad\", \n",
      "                \"name\": \"Germany\", \n",
      "                \"sort-name\": \"Germany\"\n",
      "            }, \n",
      "            \"country\": \"DE\", \n",
      "            \"disambiguation\": \"Hardcore/Gabber DJ Team\", \n",
      "            \"id\": \"06a2966d-95c1-4e20-a4d6-26e15d1d7e69\", \n",
      "            \"life-span\": {\n",
      "                \"begin\": \"2010\", \n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Tensor & Re-Direction\", \n",
      "            \"score\": \"34\", \n",
      "            \"sort-name\": \"Tensor & Re-Direction\", \n",
      "            \"type\": \"Group\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"982461f8-d0e0-44c0-9aa3-9ce9df28c71a\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Moving in The Right Direction\", \n",
      "            \"score\": \"31\", \n",
      "            \"sort-name\": \"Moving in The Right Direction\"\n",
      "        }, \n",
      "        {\n",
      "            \"disambiguation\": \"Church choir at Lourdes\", \n",
      "            \"id\": \"c2485094-3ac9-4632-b8d8-aacae7df3b6f\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Chorale et assembl\\u00e9e sous la direction de Jean Zune\", \n",
      "            \"score\": \"30\", \n",
      "            \"sort-name\": \"Chorale et assembl\\u00e9e sous la direction de Jean Zune\"\n",
      "        }, \n",
      "        {\n",
      "            \"id\": \"043456b7-0156-4a8d-9591-3018af75ced1\", \n",
      "            \"life-span\": {\n",
      "                \"ended\": null\n",
      "            }, \n",
      "            \"name\": \"Orchestre Symphonique National direction Wal-Berg\", \n",
      "            \"score\": \"28\", \n",
      "            \"sort-name\": \"Orchestre Symphonique National direction Wal-Berg\"\n",
      "        }\n",
      "    ], \n",
      "    \"count\": 1559, \n",
      "    \"created\": \"2016-03-04T11:02:38.999Z\", \n",
      "    \"offset\": 0\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# To experiment with this code freely you will have to run this code locally.\n",
    "# Take a look at the main() function for an example of how to use the code.\n",
    "# We have provided example json output in the other code editor tabs for you to\n",
    "# look at, but you will not be able to run any queries through our UI.\n",
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "BASE_URL = \"http://musicbrainz.org/ws/2/\"\n",
    "ARTIST_URL = BASE_URL + \"artist/\"\n",
    "\n",
    "# query parameters are given to the requests.get function as a dictionary; this\n",
    "# variable contains some starter parameters.\n",
    "query_type = {  \"simple\": {},\n",
    "                \"atr\": {\"inc\": \"aliases+tags+ratings\"},\n",
    "                \"aliases\": {\"inc\": \"aliases\"},\n",
    "                \"releases\": {\"inc\": \"releases\"}}\n",
    "\n",
    "\n",
    "def query_site(url, params, uid=\"\", fmt=\"json\"):\n",
    "    # This is the main function for making queries to the musicbrainz API.\n",
    "    # A json document should be returned by the query.\n",
    "    params[\"fmt\"] = fmt\n",
    "    r = requests.get(url + uid, params=params)\n",
    "    print \"requesting\", r.url\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "def query_by_name(url, params, name):\n",
    "    # This adds an artist name to the query parameters before making\n",
    "    # an API call to the function above.\n",
    "    params[\"query\"] = \"artist:\" + name\n",
    "    return query_site(url, params)\n",
    "\n",
    "\n",
    "def pretty_print(data, indent=4):\n",
    "    # After we get our output, we can format it to be more readable\n",
    "    # by using this function.\n",
    "    if type(data) == dict:\n",
    "        print json.dumps(data, indent=indent, sort_keys=True)\n",
    "    else:\n",
    "        print data\n",
    "\n",
    "\n",
    "def main():\n",
    "    '''\n",
    "    Modify the function calls and indexing below to answer the questions on\n",
    "    the next quiz. HINT: Note how the output we get from the site is a\n",
    "    multi-level JSON document, so try making print statements to step through\n",
    "    the structure one level at a time or copy the output to a separate output\n",
    "    file.\n",
    "    '''\n",
    "    results = query_by_name(ARTIST_URL, query_type[\"simple\"], \"One Direction\")\n",
    "    pretty_print(results)\n",
    "\n",
    "    #artist_id = results[\"artists\"][1][\"id\"]\n",
    "    #print \"\\nARTIST:\"\n",
    "    #pretty_print(results[\"artists\"][1])\n",
    "\n",
    "    #artist_data = query_site(ARTIST_URL, query_type[\"releases\"], artist_id)\n",
    "    #releases = artist_data[\"releases\"]\n",
    "    #print \"\\nONE RELEASE:\"\n",
    "    #pretty_print(releases[0], indent=2)\n",
    "    #release_titles = [r[\"title\"] for r in releases]\n",
    "\n",
    "    #print \"\\nALL TITLES:\"\n",
    "    #for t in release_titles:\n",
    "    #   print t\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your task is to process the supplied file and use the csv module to extract data from it.\n",
    "The data comes from NREL (National Renewable Energy Laboratory) website. Each file\n",
    "contains information from one meteorological station, in particular - about amount of\n",
    "solar and wind energy for each hour of day.\n",
    "\n",
    "Note that the first line of the datafile is neither data entry, nor header. It is a line\n",
    "describing the data source. You should extract the name of the station from it.\n",
    "\n",
    "The data should be returned as a list of lists (not dictionaries).\n",
    "You can use the csv modules \"reader\" method to get data in such format.\n",
    "Another useful method is next() - to get the next line from the iterator.\n",
    "You should only change the parse_file function.\n",
    "\"\"\"\n",
    "import csv\n",
    "import os\n",
    "\n",
    "DATADIR = \"\"\n",
    "DATAFILE = \"745090.csv\"\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    name = \"\"\n",
    "    data = []\n",
    "    with open(datafile,'rb') as f:\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        name = reader.next()[1]\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            data.append(row)\n",
    "    # Do not change the line below\n",
    "    return (name, data)\n",
    "\n",
    "\n",
    "def test():\n",
    "    datafile = os.path.join(DATADIR, DATAFILE)\n",
    "    name, data = parse_file(datafile)\n",
    "\n",
    "    assert name == \"MOUNTAIN VIEW MOFFETT FLD NAS\"\n",
    "    assert data[0][1] == \"01:00\"\n",
    "    assert data[2][0] == \"01/01/2005\"\n",
    "    assert data[2][5] == \"2\"\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COAST\n",
      "EAST\n",
      "FAR_WEST\n",
      "NORTH\n",
      "NORTH_C\n",
      "SOUTHERN\n",
      "SOUTH_C\n",
      "WEST\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Find the time and value of max load for each of the regions\n",
    "# COAST, EAST, FAR_WEST, NORTH, NORTH_C, SOUTHERN, SOUTH_C, WEST\n",
    "# and write the result out in a csv file, using pipe character | as the delimiter.\n",
    "# An example output can be seen in the \"example.csv\" file.\n",
    "\n",
    "import xlrd\n",
    "import os\n",
    "import csv\n",
    "from zipfile import ZipFile\n",
    "\n",
    "datafile = \"2013_ERCOT_Hourly_Load_Data.xls\"\n",
    "outfile = \"2013_Max_Loads.csv\"\n",
    "\n",
    "\n",
    "def open_zip(datafile):\n",
    "    with ZipFile('{0}.zip'.format(datafile), 'r') as myzip:\n",
    "        myzip.extractall()\n",
    "\n",
    "\n",
    "def parse_file(datafile):\n",
    "    workbook = xlrd.open_workbook(datafile)\n",
    "    sheet = workbook.sheet_by_index(0)\n",
    "    # sheet_data = [[sheet.cell_value(r, col) for col in range(sheet.ncols)] for r in range(sheet.nrows)]\n",
    "    # YOUR CODE HERE\n",
    "    # Remember that you can use xlrd.xldate_as_tuple(sometime, 0) to convert\n",
    "    # Excel date to Python tuple of (year, month, day, hour, minute, second)\n",
    "    data = []  \n",
    "    for col in range(1, sheet.ncols-1):\n",
    "        coast_data = sheet.col_values(col, start_rowx=1, end_rowx=None)\n",
    "        idx_maxvalue = coast_data.index(max(coast_data))\n",
    "        Year, Month, Day, Hour, Min, Sec = xlrd.xldate_as_tuple(sheet.cell_value(idx_maxvalue+1, 0), 0)\n",
    "        print sheet.cell_value(0, col)\n",
    "        station = {\n",
    "            'Station': sheet.cell_value(0, col),\n",
    "            'Year': Year,\n",
    "            'Month': Month,\n",
    "            'Day': Day,\n",
    "            'Hour': Hour,\n",
    "            'Max Load': max(coast_data)\n",
    "        }\n",
    "        data.append(station)\n",
    "    return data\n",
    "\n",
    "def save_file(data, filename):\n",
    "    # YOUR CODE HERE\n",
    "    with open(filename,'w') as csvfile:\n",
    "        fieldnames = ['Station', 'Year', 'Month', 'Day', 'Hour', 'Max Load']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames= fieldnames, delimiter=\"|\")\n",
    "        writer.writeheader()\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "    \n",
    "def test():\n",
    "    #open_zip(datafile)\n",
    "    data = parse_file(datafile)\n",
    "    save_file(data, outfile)\n",
    "\n",
    "    number_of_rows = 0\n",
    "    stations = []\n",
    "\n",
    "    ans = {'FAR_WEST': {'Max Load': '2281.2722140000024',\n",
    "                        'Year': '2013',\n",
    "                        'Month': '6',\n",
    "                        'Day': '26',\n",
    "                        'Hour': '17'}}\n",
    "    correct_stations = ['COAST', 'EAST', 'FAR_WEST', 'NORTH',\n",
    "                        'NORTH_C', 'SOUTHERN', 'SOUTH_C', 'WEST']\n",
    "    fields = ['Year', 'Month', 'Day', 'Hour', 'Max Load']\n",
    "\n",
    "    with open(outfile) as of:\n",
    "        csvfile = csv.DictReader(of, delimiter=\"|\")\n",
    "        for line in csvfile:\n",
    "            station = line['Station']\n",
    "            if station == 'FAR_WEST':\n",
    "                for field in fields:\n",
    "                    # Check if 'Max Load' is within .1 of answer\n",
    "                    if field == 'Max Load':\n",
    "                        max_answer = round(float(ans[station][field]), 1)\n",
    "                        max_line = round(float(line[field]), 1)\n",
    "                        assert max_answer == max_line\n",
    "\n",
    "                    # Otherwise check for equality\n",
    "                    else:\n",
    "                        assert ans[station][field] == line[field]\n",
    "\n",
    "            number_of_rows += 1\n",
    "            stations.append(station)\n",
    "\n",
    "        # Output should be 8 lines not including header\n",
    "        assert number_of_rows == 8\n",
    "\n",
    "        # Check Station Names\n",
    "        assert set(stations) == set(correct_stations)\n",
    "\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "This exercise shows some important concepts that you should be aware about:\n",
    "- using codecs module to write unicode files\n",
    "- using authentication with web APIs\n",
    "- using offset when accessing web APIs\n",
    "\n",
    "To run this code locally you have to register at the NYTimes developer site \n",
    "and get your own API key. You will be able to complete this exercise in our UI without doing so,\n",
    "as we have provided a sample result.\n",
    "\n",
    "Your task is to process the saved file that represents the most popular (by view count)\n",
    "articles in the last day, and return the following data:\n",
    "- list of dictionaries, where the dictionary key is \"section\" and value is \"title\"\n",
    "- list of URLs for all media entries with \"format\": \"Standard Thumbnail\"\n",
    "\n",
    "All your changes should be in the article_overview function.\n",
    "The rest of functions are provided for your convenience, if you want to access the API by yourself.\n",
    "\"\"\"\n",
    "import json\n",
    "import codecs\n",
    "import requests\n",
    "\n",
    "URL_MAIN = \"http://api.nytimes.com/svc/\"\n",
    "URL_POPULAR = URL_MAIN + \"mostpopular/v2/\"\n",
    "API_KEY = { \"popular\": \"\",\n",
    "            \"article\": \"\"}\n",
    "\n",
    "\n",
    "def get_from_file(kind, period):\n",
    "    filename = \"popular-{0}-{1}.json\".format(kind, period)\n",
    "    with open(filename, \"r\") as f:\n",
    "        return json.loads(f.read())\n",
    "\n",
    "\n",
    "def article_overview(kind, period):\n",
    "    data = get_from_file(kind, period)\n",
    "    titles = []\n",
    "    urls =[]\n",
    "    # YOUR CODE HERE\n",
    "    titles = [{r[\"section\"]: r[\"title\"]} for r in data]\n",
    "    for data_record in data:\n",
    "        for media in data_record[\"media\"]:\n",
    "            for metadata in media[\"media-metadata\"]:\n",
    "                if metadata[\"format\"] == \"Standard Thumbnail\":\n",
    "                    urls.append(metadata[\"url\"])\n",
    "    return (titles, urls)\n",
    "\n",
    "\n",
    "def query_site(url, target, offset):\n",
    "    # This will set up the query with the API key and offset\n",
    "    # Web services often use offset paramter to return data in small chunks\n",
    "    # NYTimes returns 20 articles per request, if you want the next 20\n",
    "    # You have to provide the offset parameter\n",
    "    if API_KEY[\"popular\"] == \"\" or API_KEY[\"article\"] == \"\":\n",
    "        print \"You need to register for NYTimes Developer account to run this program.\"\n",
    "        print \"See Intructor notes for information\"\n",
    "        return False\n",
    "    params = {\"api-key\": API_KEY[target], \"offset\": offset}\n",
    "    r = requests.get(url, params = params)\n",
    "\n",
    "    if r.status_code == requests.codes.ok:\n",
    "        return r.json()\n",
    "    else:\n",
    "        r.raise_for_status()\n",
    "\n",
    "\n",
    "def get_popular(url, kind, days, section=\"all-sections\", offset=0):\n",
    "    # This function will construct the query according to the requirements of the site\n",
    "    # and return the data, or print an error message if called incorrectly\n",
    "    if days not in [1,7,30]:\n",
    "        print \"Time period can be 1,7, 30 days only\"\n",
    "        return False\n",
    "    if kind not in [\"viewed\", \"shared\", \"emailed\"]:\n",
    "        print \"kind can be only one of viewed/shared/emailed\"\n",
    "        return False\n",
    "\n",
    "    url += \"most{0}/{1}/{2}.json\".format(kind, section, days)\n",
    "    data = query_site(url, \"popular\", offset)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_file(kind, period):\n",
    "    # This will process all results, by calling the API repeatedly with supplied offset value,\n",
    "    # combine the data and then write all results in a file.\n",
    "    data = get_popular(URL_POPULAR, \"viewed\", 1)\n",
    "    num_results = data[\"num_results\"]\n",
    "    full_data = []\n",
    "    with codecs.open(\"popular-{0}-{1}.json\".format(kind, period), encoding='utf-8', mode='w') as v:\n",
    "        for offset in range(0, num_results, 20):        \n",
    "            data = get_popular(URL_POPULAR, kind, period, offset=offset)\n",
    "            full_data += data[\"results\"]\n",
    "        \n",
    "        v.write(json.dumps(full_data, indent=2))\n",
    "\n",
    "\n",
    "def test():\n",
    "    titles, urls = article_overview(\"viewed\", 1)\n",
    "    assert len(titles) == 20\n",
    "    assert len(urls) == 30\n",
    "    assert titles[2] == {'Opinion': 'Professors, We Need You!'}\n",
    "    assert urls[20] == 'http://graphics8.nytimes.com/images/2014/02/17/sports/ICEDANCE/ICEDANCE-thumbStandard.jpg'\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your task here is to extract data from xml on authors of an article\n",
    "# and add it to a list, one item for an author.\n",
    "# See the provided data structure for the expected format.\n",
    "# The tags for first name, surname and email should map directly\n",
    "# to the dictionary keys\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "article_file = \"exampleResearchArticle.xml\"\n",
    "\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "def get_authors(root):\n",
    "    authors = []\n",
    "    for author in root.findall('./fm/bibl/aug/au'):\n",
    "        data = {\n",
    "                \"fnm\": author.find('fnm').text,\n",
    "                \"snm\": author.find('snm').text,\n",
    "                \"email\": author.find('email').text\n",
    "        }\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "        authors.append(data)\n",
    "\n",
    "    return authors\n",
    "\n",
    "\n",
    "def test():\n",
    "    solution = [{'fnm': 'Omer', 'snm': 'Mei-Dan', 'email': 'omer@extremegate.com'}, {'fnm': 'Mike', 'snm': 'Carmont', 'email': 'mcarmont@hotmail.com'}, {'fnm': 'Lior', 'snm': 'Laver', 'email': 'laver17@gmail.com'}, {'fnm': 'Meir', 'snm': 'Nyska', 'email': 'nyska@internet-zahav.net'}, {'fnm': 'Hagay', 'snm': 'Kammar', 'email': 'kammarh@gmail.com'}, {'fnm': 'Gideon', 'snm': 'Mann', 'email': 'gideon.mann.md@gmail.com'}, {'fnm': 'Barnaby', 'snm': 'Clarck', 'email': 'barns.nz@gmail.com'}, {'fnm': 'Eugene', 'snm': 'Kots', 'email': 'eukots@gmail.com'}]\n",
    "    \n",
    "    root = get_root(article_file)\n",
    "    data = get_authors(root)\n",
    "\n",
    "    assert data[0] == solution[0]\n",
    "    assert data[1][\"fnm\"] == solution[1][\"fnm\"]\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your task here is to extract data from xml on authors of an article\n",
    "# and add it to a list, one item for an author.\n",
    "# See the provided data structure for the expected format.\n",
    "# The tags for first name, surname and email should map directly\n",
    "# to the dictionary keys, but you have to extract the attributes from the \"insr\" tag\n",
    "# and add them to the list for the dictionary key \"insr\"\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "article_file = \"exampleResearchArticle.xml\"\n",
    "\n",
    "\n",
    "def get_root(fname):\n",
    "    tree = ET.parse(fname)\n",
    "    return tree.getroot()\n",
    "\n",
    "\n",
    "def get_authors(root):\n",
    "    authors = []\n",
    "    for author in root.findall('./fm/bibl/aug/au'):\n",
    "        data = {\n",
    "                \"fnm\": None,\n",
    "                \"snm\": None,\n",
    "                \"email\": None,\n",
    "                \"insr\": []\n",
    "        }\n",
    "\n",
    "        # YOUR CODE HERE\n",
    "        data[\"fnm\"] = author.find('./fnm').text\n",
    "        data[\"snm\"] = author.find('./snm').text\n",
    "        data[\"email\"] = author.find('./email').text\n",
    "        for insr in author.findall('./insr'):\n",
    "            #print insr.attrib[\"iid\"]\n",
    "            data[\"insr\"].append(insr.attrib[\"iid\"])\n",
    "\n",
    "        authors.append(data)\n",
    "\n",
    "    return authors\n",
    "\n",
    "\n",
    "def test():\n",
    "    solution = [{'insr': ['I1'], 'fnm': 'Omer', 'snm': 'Mei-Dan', 'email': 'omer@extremegate.com'},\n",
    "                {'insr': ['I2'], 'fnm': 'Mike', 'snm': 'Carmont', 'email': 'mcarmont@hotmail.com'},\n",
    "                {'insr': ['I3', 'I4'], 'fnm': 'Lior', 'snm': 'Laver', 'email': 'laver17@gmail.com'},\n",
    "                {'insr': ['I3'], 'fnm': 'Meir', 'snm': 'Nyska', 'email': 'nyska@internet-zahav.net'},\n",
    "                {'insr': ['I8'], 'fnm': 'Hagay', 'snm': 'Kammar', 'email': 'kammarh@gmail.com'},\n",
    "                {'insr': ['I3', 'I5'], 'fnm': 'Gideon', 'snm': 'Mann', 'email': 'gideon.mann.md@gmail.com'},\n",
    "                {'insr': ['I6'], 'fnm': 'Barnaby', 'snm': 'Clarck', 'email': 'barns.nz@gmail.com'},\n",
    "                {'insr': ['I7'], 'fnm': 'Eugene', 'snm': 'Kots', 'email': 'eukots@gmail.com'}]\n",
    "\n",
    "    root = get_root(article_file)\n",
    "    data = get_authors(root)\n",
    "\n",
    "    assert data[0] == solution[0]\n",
    "    assert data[1][\"insr\"] == solution[1][\"insr\"]\n",
    "\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def options(soup, id):\n",
    "    option_values = []\n",
    "    carrier_list = soup.find(id=id)\n",
    "    for option in carrier_list.find_all('option'):\n",
    "        option_values.append(option['value'])\n",
    "    return option_values\n",
    "\n",
    "def print_list(label, codes):\n",
    "    print \"\\n%s:\" % label\n",
    "    for c in codes:\n",
    "        print c\n",
    "        \n",
    "def main():\n",
    "    soup = BeautifulSoup(open(\"Data_Elements.htm\"))\n",
    "    \n",
    "    codes = options(soup, 'CarrierList')\n",
    "    print_list(\"Carriers\", codes)\n",
    "    \n",
    "    codes = options(soup, 'AirportList')\n",
    "    print_list(\"Airports\", codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "s = requests.Session()\n",
    "\n",
    "r = s.get(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\")\n",
    "soup = BeautifulSoup(r.text)\n",
    "viewstate_element = soup.find(id=\"__VIEWSTATE\")\n",
    "viewstate = viewstate_element[\"value\"]\n",
    "eventvalidation_element = soup.find(id=\"__EVENTVALIDATION\")\n",
    "eventvalidation = eventvalidation_element[\"value\"]\n",
    "\n",
    "r = s.post(\"http://www.transtats.bts.gov/Data_Elements.aspx?Data=2\",\n",
    "          data={'AirportList':\"BOS\",\n",
    "                'CarrierList':\"VX\",\n",
    "                'Submit':\"Submit\",\n",
    "                '__EVENTTARGET':\"\",\n",
    "                '__EVENTARGUMENT':\"\",\n",
    "                '__EVENTVALIDATION':eventvalidation,\n",
    "                '__VIEWSTATE':viewstate})\n",
    "f = open(\"virgin_and_logan_airport.html\", \"w\")\n",
    "f.write(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
